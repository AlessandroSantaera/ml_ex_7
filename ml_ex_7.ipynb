{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c5d08680","cell_type":"markdown","source":"# **Reti Neurali** ","metadata":{}},{"id":"52a5f30f","cell_type":"markdown","source":"## **Esercizio 1: Download e pre-processamento dei dati.**\n\nScaricare e pre-processare i dati per il successivo addestramento del modello. \n\nIl dataset che utilizzeremo sarà CIFAR10, scaricabile dalla libreria `tensorflow.keras.datasets`","metadata":{}},{"id":"42e9791d","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\n\n# Download dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n# Stampare le shape dei dati","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:33:13.797071Z","iopub.execute_input":"2025-05-27T08:33:13.797844Z","iopub.status.idle":"2025-05-27T08:33:16.837022Z","shell.execute_reply.started":"2025-05-27T08:33:13.797818Z","shell.execute_reply":"2025-05-27T08:33:16.836081Z"}},"outputs":[{"name":"stdout","text":"(50000, 32, 32, 3)\n(50000,)\n(10000, 32, 32, 3)\n(10000,)\n","output_type":"stream"}],"execution_count":39},{"id":"4d6a0ca6","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Pre-processamento dei dati\n\n# svolgimento...\nx_train = x_train.reshape(x_train.shape[0], -1)\nx_test = x_test.reshape(x_test.shape[0], -1)\nscaler = StandardScaler()\n\nx_train_std = scaler.fit_transform(x_train)\nx_test_std = scaler.transform(x_test)\nprint(x_train_std.shape)\nprint(x_test_std.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:33:16.838351Z","iopub.execute_input":"2025-05-27T08:33:16.838601Z","iopub.status.idle":"2025-05-27T08:33:20.402357Z","shell.execute_reply.started":"2025-05-27T08:33:16.838583Z","shell.execute_reply":"2025-05-27T08:33:20.401496Z"}},"outputs":[{"name":"stdout","text":"(50000, 3072)\n(10000, 3072)\n","output_type":"stream"}],"execution_count":40},{"id":"5103ad7f","cell_type":"markdown","source":"## **Esercizio 2: Creare modello MLP**\n\nPer creare il modello MLP utilizziamo l' oggetto `MLPClassifier` dal modulo `sklearn.neural_networks`. Questo è un oggetto molto complesso che prevede la possibilità di specificare tanti parametri, permettendoci una personalizzazione molto dettagliata. Vediamo di seguito gli argomenti principali:\n\n- `hidden_layer_sizes`: rappresenta la struttura dell' MLP, sotto forma di una tupla. La tupla deve essere composta da numeri interi, ogni numero indica il numero di neuroni presenti nel rispettivo layer.\n\nEsempio: \n\n`hidden_layer_sizes` = `(100)`\n\ncreerà un solo layer con 100 neuroni\n\n`hidden_layer_sizes` = `(100, 50)`\n\ncreerà due layer, il primo con 100 neuroni, il secondo invece con 50.\n\n- `max_iter`: massimo numero di iterazioni per raggiungere la convergenza. \n\n- `activation`: indica quale funzione di attivazione utilizzare, valori possibili sono `'relu'`, `'logistic'`, `'tanh'` and `'identity'`.\n\n- `solver`: indica quale algoritmo di ottimizzazione utilizzare, valori possibili sono `'adam'`, `'sgd'` and `'lbfgs'`.\n\n- `learning_rate_init`: valore iniziale del learning rate.\n\n- `verbose`: valore booleano che, se impostato su `True`, stampa l' output di ogni iterazione di training. Molto utile per monitorare il training.\n\n- `random_state`: fissa il seed della randomizzazione.\n","metadata":{}},{"id":"b0f857d0","cell_type":"markdown","source":"Per iniziare creiamo un MLP molto basilare, alleniamolo e testiamone le performance. Come parametri utilizzeremo:\n\n- `hidden_layer_sizes` = `(100)`\n\n- `max_iter` = `20`\n\n- `random_state` = `42`","metadata":{}},{"id":"4650894a","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP \n\n# svolgimento...\nmlp= MLPClassifier(hidden_layer_sizes = (100),max_iter = 20, random_state = 42 )\n\n# Allenare MLP\n\n# svolgimento...\n\nmlp.fit(x_train_std, y_train)\n# Valutare MLP\n\n# svolgimento...\ntrain_predict= mlp.predict(x_train_std)\ntest_predict= mlp.predict(x_test_std)\n\n# Stampare l' accuratezza\n\n# svolgimento...\naccuracy_train= accuracy_score(y_train, train_predict)\naccuracy_test= accuracy_score(y_test, test_predict)\nprint(\"Accuracy del train: \",accuracy_train)\nprint(\"Accuracy del test: \", accuracy_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:33:20.403316Z","iopub.execute_input":"2025-05-27T08:33:20.403549Z","iopub.status.idle":"2025-05-27T08:34:16.999528Z","shell.execute_reply.started":"2025-05-27T08:33:20.403531Z","shell.execute_reply":"2025-05-27T08:34:16.998340Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy del train:  0.71198\nAccuracy del test:  0.5057\n","output_type":"stream"}],"execution_count":41},{"id":"6171280e","cell_type":"markdown","source":"## **Esercizio 2.1: Aumentiamo i parametri del nostro modello**\n\nProviamo adesso ad aumentare i dettagli del nostro modello, modificando o aggiungendo i parametri sopra specificati. ","metadata":{}},{"id":"f35afa68","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP con più strati e altre specifiche\n\n# svolgimento...\nmlp= MLPClassifier(hidden_layer_sizes = (100,50),max_iter = 50, random_state = 42,verbose= True, activation='logistic', solver='sgd' )\n\nmlp.fit(x_train_std, y_train)\n\ntrain_predict=mlp.predict(x_train_std)\ntest_predict=mlp.predict(x_test_std)\n\naccuracy_train=accuracy_score(y_train, train_predict)\naccuracy_test=accuracy_score(y_test, test_predict)\n\nprint(\"Accuracy del train: \",accuracy_train)\nprint(\"Accuracy del test: \", accuracy_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:34:17.001351Z","iopub.execute_input":"2025-05-27T08:34:17.001614Z","iopub.status.idle":"2025-05-27T08:36:23.379833Z","shell.execute_reply.started":"2025-05-27T08:34:17.001594Z","shell.execute_reply":"2025-05-27T08:36:23.378968Z"}},"outputs":[{"name":"stdout","text":"Iteration 1, loss = 2.29411142\nIteration 2, loss = 2.25892467\nIteration 3, loss = 2.23094289\nIteration 4, loss = 2.19986364\nIteration 5, loss = 2.16612044\nIteration 6, loss = 2.13210478\nIteration 7, loss = 2.10018741\nIteration 8, loss = 2.07131360\nIteration 9, loss = 2.04606504\nIteration 10, loss = 2.02407949\nIteration 11, loss = 2.00496407\nIteration 12, loss = 1.98816671\nIteration 13, loss = 1.97316024\nIteration 14, loss = 1.95958031\nIteration 15, loss = 1.94734572\nIteration 16, loss = 1.93607822\nIteration 17, loss = 1.92569921\nIteration 18, loss = 1.91603717\nIteration 19, loss = 1.90735138\nIteration 20, loss = 1.89893509\nIteration 21, loss = 1.89115769\nIteration 22, loss = 1.88351279\nIteration 23, loss = 1.87622130\nIteration 24, loss = 1.86914419\nIteration 25, loss = 1.86214122\nIteration 26, loss = 1.85527961\nIteration 27, loss = 1.84859691\nIteration 28, loss = 1.84205296\nIteration 29, loss = 1.83552843\nIteration 30, loss = 1.82920156\nIteration 31, loss = 1.82298501\nIteration 32, loss = 1.81684709\nIteration 33, loss = 1.81091774\nIteration 34, loss = 1.80522616\nIteration 35, loss = 1.79958778\nIteration 36, loss = 1.79417563\nIteration 37, loss = 1.78893908\nIteration 38, loss = 1.78388129\nIteration 39, loss = 1.77898231\nIteration 40, loss = 1.77426186\nIteration 41, loss = 1.76968682\nIteration 42, loss = 1.76512665\nIteration 43, loss = 1.76085870\nIteration 44, loss = 1.75669778\nIteration 45, loss = 1.75261341\nIteration 46, loss = 1.74854899\nIteration 47, loss = 1.74466951\nIteration 48, loss = 1.74083227\nIteration 49, loss = 1.73690227\nIteration 50, loss = 1.73318811\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy del train:  0.38792\nAccuracy del test:  0.3857\n","output_type":"stream"}],"execution_count":42},{"id":"226b1681","cell_type":"markdown","source":"## **Esercizio 3: Implementare manualmente l' algoritmo di early stopping.**\n\nL' algoritmo di early stopping ci permette di terminare anticipatamente l' allenamento di un modello nel caso in cui questo raggiunga la convergenza. Supponiamo infatti che il nostro modello raggiunga un certo livello di accuratezza e che non riesca a migliorare oltre quel livello. Questo significa che il modello, da quel momento in poi, non sta più apprendendo nuove informazioni, per cui le successive iterazioni sono superflue, ed inoltre rischiano di essere dannose, spingendo il modello verso l' overfitting. \n\nL' early stopping verifica ad ogni iterazione che l' accuratezza del modello sia incrementata di una certa tolleranza. Se questa tolleranza non viene superata per un certo numero di epoche, allora possiamo decidere di stoppare l' allenamento in quanto il modello ha raggiunto la convergenza.\n\n**N.B: per applicare early stopping è necessario specificare i seguenti parametri dell' MLP:**\n\n- `warm_start`=`True` in modo che il training proceda dallo stato attuale del modello e non dall' inizializzazione.\n\n- `max_iter`=`1` in modo che il modello venga allenato per una sola epoca. Per l' early stopping infatti dovremo gestire manualmente il numero di iterazioni.","metadata":{}},{"id":"1168be12-2830-4981-aab6-eadb6c079926","cell_type":"code","source":"train_fraction = 0.8  \nval_fraction = 0.2\n\nnum_train = int(train_fraction * x_train.shape[0])\nx_train_1 = x_train_std[:num_train]\ny_train_1 = y_train[:num_train]\n\nx_val = x_train_std[num_train:]\ny_val = y_train[num_train:]\n\nprint(f\"Training set: {x_train_1.shape}, {y_train_1.shape}\")\nprint(f\"Validation set: {x_val.shape}, {y_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:36:23.380895Z","iopub.execute_input":"2025-05-27T08:36:23.381206Z","iopub.status.idle":"2025-05-27T08:36:23.386769Z","shell.execute_reply.started":"2025-05-27T08:36:23.381178Z","shell.execute_reply":"2025-05-27T08:36:23.386060Z"}},"outputs":[{"name":"stdout","text":"Training set: (40000, 3072), (40000,)\nValidation set: (10000, 3072), (10000,)\n","output_type":"stream"}],"execution_count":43},{"id":"8ee1709d","cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\n\nn_total_epochs = 100  \npatience = 10         \ntolerance = 1e-4      \n\nbest_val_accuracy = 0.0\nepochs_without_improvement = 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:36:23.387831Z","iopub.execute_input":"2025-05-27T08:36:23.388138Z","iopub.status.idle":"2025-05-27T08:36:23.404914Z","shell.execute_reply.started":"2025-05-27T08:36:23.388095Z","shell.execute_reply":"2025-05-27T08:36:23.403774Z"}},"outputs":[],"execution_count":44},{"id":"9ddace1e","cell_type":"code","source":"mlp= MLPClassifier(hidden_layer_sizes = (100,50),max_iter = 1, random_state = 42, warm_start=True, verbose=True)\n\nfor epoca in range(n_total_epochs):\n    mlp.fit(x_train_std, y_train)\n\n    val_predict=mlp.predict(x_val)\n    val_accuracy=accuracy_score(y_val, val_predict)\n\n    print(f\"Epoca{epoca}, accuracy sulla validation: {val_accuracy:.4f}\")\n\n    if val_accuracy> best_val_accuracy + tolerance:\n        best_val_accuracy= val_accuracy\n        epochs_without_improvement=0\n    else:\n        epochs_without_improvement +=1\n\n    if epochs_without_improvement >= patience:\n        print(f\"Early stopping all'epoca {epoca+1}\")\n        break\ntest_predict= mlp.predict(x_test_std)\ntest_accuracy= accuracy_score(y_test,test_predict)\n\nprint(\"Accuracy finale sul test:\", test_accuracy)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:36:23.405892Z","iopub.execute_input":"2025-05-27T08:36:23.406193Z","iopub.status.idle":"2025-05-27T08:40:55.031751Z","shell.execute_reply.started":"2025-05-27T08:36:23.406172Z","shell.execute_reply":"2025-05-27T08:40:55.030792Z"}},"outputs":[{"name":"stdout","text":"Iteration 1, loss = 1.75953688\nEpoca0, accuracy sulla validation: 0.4552\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2, loss = 1.56217223\nEpoca1, accuracy sulla validation: 0.4887\nIteration 3, loss = 1.44992466\nEpoca2, accuracy sulla validation: 0.5110\nIteration 4, loss = 1.36972870\nEpoca3, accuracy sulla validation: 0.5352\nIteration 5, loss = 1.30807250\nEpoca4, accuracy sulla validation: 0.5496\nIteration 6, loss = 1.25765208\nEpoca5, accuracy sulla validation: 0.5643\nIteration 7, loss = 1.21291055\nEpoca6, accuracy sulla validation: 0.5813\nIteration 8, loss = 1.17317639\nEpoca7, accuracy sulla validation: 0.5871\nIteration 9, loss = 1.14145858\nEpoca8, accuracy sulla validation: 0.5978\nIteration 10, loss = 1.11246592\nEpoca9, accuracy sulla validation: 0.6039\nIteration 11, loss = 1.08802703\nEpoca10, accuracy sulla validation: 0.6154\nIteration 12, loss = 1.06296634\nEpoca11, accuracy sulla validation: 0.6189\nIteration 13, loss = 1.03885621\nEpoca12, accuracy sulla validation: 0.6248\nIteration 14, loss = 1.01714102\nEpoca13, accuracy sulla validation: 0.6301\nIteration 15, loss = 0.99748605\nEpoca14, accuracy sulla validation: 0.6345\nIteration 16, loss = 0.97878105\nEpoca15, accuracy sulla validation: 0.6383\nIteration 17, loss = 0.95797839\nEpoca16, accuracy sulla validation: 0.6389\nIteration 18, loss = 0.93786055\nEpoca17, accuracy sulla validation: 0.6483\nIteration 19, loss = 0.92388436\nEpoca18, accuracy sulla validation: 0.6470\nIteration 20, loss = 0.91035799\nEpoca19, accuracy sulla validation: 0.6517\nIteration 21, loss = 0.89407008\nEpoca20, accuracy sulla validation: 0.6490\nIteration 22, loss = 0.88282765\nEpoca21, accuracy sulla validation: 0.6563\nIteration 23, loss = 0.86673136\nEpoca22, accuracy sulla validation: 0.6601\nIteration 24, loss = 0.85119735\nEpoca23, accuracy sulla validation: 0.6629\nIteration 25, loss = 0.84895275\nEpoca24, accuracy sulla validation: 0.6670\nIteration 26, loss = 0.83521378\nEpoca25, accuracy sulla validation: 0.6692\nIteration 27, loss = 0.82843782\nEpoca26, accuracy sulla validation: 0.6714\nIteration 28, loss = 0.81488450\nEpoca27, accuracy sulla validation: 0.6697\nIteration 29, loss = 0.80848258\nEpoca28, accuracy sulla validation: 0.6735\nIteration 30, loss = 0.79792771\nEpoca29, accuracy sulla validation: 0.6780\nIteration 31, loss = 0.78815180\nEpoca30, accuracy sulla validation: 0.6741\nIteration 32, loss = 0.77644010\nEpoca31, accuracy sulla validation: 0.6814\nIteration 33, loss = 0.76621948\nEpoca32, accuracy sulla validation: 0.6793\nIteration 34, loss = 0.76461865\nEpoca33, accuracy sulla validation: 0.6840\nIteration 35, loss = 0.74852407\nEpoca34, accuracy sulla validation: 0.6764\nIteration 36, loss = 0.74373252\nEpoca35, accuracy sulla validation: 0.6816\nIteration 37, loss = 0.73799962\nEpoca36, accuracy sulla validation: 0.6872\nIteration 38, loss = 0.73260293\nEpoca37, accuracy sulla validation: 0.6863\nIteration 39, loss = 0.72112646\nEpoca38, accuracy sulla validation: 0.6825\nIteration 40, loss = 0.71524595\nEpoca39, accuracy sulla validation: 0.6847\nIteration 41, loss = 0.70892957\nEpoca40, accuracy sulla validation: 0.6761\nIteration 42, loss = 0.70811407\nEpoca41, accuracy sulla validation: 0.6821\nIteration 43, loss = 0.69383919\nEpoca42, accuracy sulla validation: 0.6890\nIteration 44, loss = 0.68790284\nEpoca43, accuracy sulla validation: 0.6889\nIteration 45, loss = 0.68146039\nEpoca44, accuracy sulla validation: 0.6877\nIteration 46, loss = 0.67308749\nEpoca45, accuracy sulla validation: 0.6914\nIteration 47, loss = 0.68045026\nEpoca46, accuracy sulla validation: 0.6949\nIteration 48, loss = 0.67313791\nEpoca47, accuracy sulla validation: 0.6991\nIteration 49, loss = 0.66897076\nEpoca48, accuracy sulla validation: 0.7034\nIteration 50, loss = 0.66248027\nEpoca49, accuracy sulla validation: 0.6999\nIteration 51, loss = 0.64790883\nEpoca50, accuracy sulla validation: 0.7049\nIteration 52, loss = 0.63670317\nEpoca51, accuracy sulla validation: 0.7067\nIteration 53, loss = 0.63480754\nEpoca52, accuracy sulla validation: 0.7109\nIteration 54, loss = 0.62608647\nEpoca53, accuracy sulla validation: 0.7103\nIteration 55, loss = 0.62319763\nEpoca54, accuracy sulla validation: 0.7157\nIteration 56, loss = 0.61566218\nEpoca55, accuracy sulla validation: 0.7147\nIteration 57, loss = 0.61670196\nEpoca56, accuracy sulla validation: 0.7114\nIteration 58, loss = 0.61014080\nEpoca57, accuracy sulla validation: 0.7146\nIteration 59, loss = 0.60845592\nEpoca58, accuracy sulla validation: 0.7166\nIteration 60, loss = 0.59533207\nEpoca59, accuracy sulla validation: 0.7251\nIteration 61, loss = 0.59771709\nEpoca60, accuracy sulla validation: 0.7213\nIteration 62, loss = 0.58932449\nEpoca61, accuracy sulla validation: 0.7178\nIteration 63, loss = 0.58986608\nEpoca62, accuracy sulla validation: 0.7205\nIteration 64, loss = 0.58919119\nEpoca63, accuracy sulla validation: 0.7181\nIteration 65, loss = 0.58548003\nEpoca64, accuracy sulla validation: 0.7316\nIteration 66, loss = 0.56341403\nEpoca65, accuracy sulla validation: 0.7258\nIteration 67, loss = 0.57268511\nEpoca66, accuracy sulla validation: 0.7266\nIteration 68, loss = 0.57537269\nEpoca67, accuracy sulla validation: 0.7360\nIteration 69, loss = 0.55603656\nEpoca68, accuracy sulla validation: 0.7384\nIteration 70, loss = 0.55943427\nEpoca69, accuracy sulla validation: 0.7285\nIteration 71, loss = 0.55416875\nEpoca70, accuracy sulla validation: 0.7338\nIteration 72, loss = 0.55477417\nEpoca71, accuracy sulla validation: 0.7336\nIteration 73, loss = 0.56006682\nEpoca72, accuracy sulla validation: 0.7303\nIteration 74, loss = 0.54037857\nEpoca73, accuracy sulla validation: 0.7391\nIteration 75, loss = 0.54805285\nEpoca74, accuracy sulla validation: 0.7393\nIteration 76, loss = 0.54166603\nEpoca75, accuracy sulla validation: 0.7334\nIteration 77, loss = 0.54439741\nEpoca76, accuracy sulla validation: 0.7374\nIteration 78, loss = 0.52851844\nEpoca77, accuracy sulla validation: 0.7361\nIteration 79, loss = 0.53104531\nEpoca78, accuracy sulla validation: 0.7374\nIteration 80, loss = 0.53798822\nEpoca79, accuracy sulla validation: 0.7375\nIteration 81, loss = 0.52453203\nEpoca80, accuracy sulla validation: 0.7391\nIteration 82, loss = 0.51899573\nEpoca81, accuracy sulla validation: 0.7347\nIteration 83, loss = 0.52488942\nEpoca82, accuracy sulla validation: 0.7374\nIteration 84, loss = 0.51883334\nEpoca83, accuracy sulla validation: 0.7325\nIteration 85, loss = 0.51446399\nEpoca84, accuracy sulla validation: 0.7247\nEarly stopping all'epoca 85\nAccuracy finale sul test: 0.4469\n","output_type":"stream"}],"execution_count":45}]}